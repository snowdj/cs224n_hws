{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I adapted the starter codes to make them compatible with python 3. All codes were run in python 3 (rather than the officially supported version of python, i.e., python 2) unless clarified. As a result, I got slightly different results with solution for some questions, but all of my results passed the sanity check. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a**. Proof: $$\\text{softmax}(x+c)_i = \\frac{e^{x_i+c}}{\\sum_je^{x_j+c}}=\\frac{e^ce^{x_i}}{e^c\\sum_je^{x_j}}\n",
    "=\\frac{e^{x_i}}{\\sum_je^{x_j}}=\\text{softmax}(x)_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[0.26894142 0.73105858]\n",
      "[[0.26894142 0.73105858]\n",
      " [0.26894142 0.73105858]]\n",
      "[[0.73105858 0.26894142]]\n",
      "You should be able to verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q1_softmax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Neural Network Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a**.\n",
    "$$\\sigma'(x)=\\frac{-1}{(1+e^{-x})^2}\\cdot(-e^{-x})=\\frac{1}{1+e^{-x}}\\cdot\\frac{e^{-x}}{1+e^{-x}}\n",
    "=\\sigma(x)(1-\\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b**. Assume that only the k-th dimension of $\\boldsymbol{y}$ is 1 and others are 0. We have\n",
    "$$CE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}})=-y_k\\log (\\hat{y}_k)=-y_k(\\log e^{\\theta_k}-\\log \\sum_i e^{\\theta_i}) = \\log \\sum_i e^{\\theta_i} - \\theta_k$$\n",
    "$$\\frac{\\partial CE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}})}{\\partial\\theta_k}\n",
    "=\\frac{e^{\\theta_k}}{\\sum_i e^{\\theta_i}}-1=\\hat{y}_k-1$$\n",
    "For $j \\neq k$,\n",
    "$$\\frac{\\partial CE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}})}{\\partial\\theta_j}\n",
    "=\\frac{e^{\\theta_j}}{\\sum_i e^{\\theta_i}}=\\hat{y}_j$$\n",
    "\n",
    "$$\\therefore \\frac{\\partial CE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}})}{\\partial\\boldsymbol{\\theta}}=\\boldsymbol{\\hat{y}}-\\boldsymbol{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c**. Denote $z_1=xW_1+b_1$ and $z_2=hW_2+b_2$, then\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial x} = (\\frac{\\partial J}{\\partial z_2}\\frac{\\partial z_2}{\\partial h}\\odot\\frac{\\partial h}{\\partial z_1})\\frac{\\partial z_1}{\\partial x}\n",
    "=((\\hat{y}-y)W_2^\\text{T}\\odot(h(1-h))W_1^\\text{T})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d**. The total number of parameters is $(D_x+1)H+(H+1)D_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[0.73105858 0.88079708]\n",
      " [0.26894142 0.11920292]]\n",
      "[[0.19661193 0.10499359]\n",
      " [0.19661193 0.10499359]]\n",
      "You should verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q2_sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2f**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q2_gradcheck.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2g**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q2_neural.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a**. We know $u_W$ and $v_w$ are column vectors of size, say $H$. The the shape of $U$ is $(H, V)$. Let $\\hat{y}$ be the column vector of softmax predictions, and $y$ be the one-hot labels as a column vector. Then,\n",
    "$$\\frac{\\partial J}{\\partial v_c}=U(\\hat{y}-y)$$\n",
    "The elements $\\hat{y}_w$ and $y_w$ of $\\hat{y}$ and $y$ are scalers. Thus equivalently,\n",
    "$$\\frac{\\partial J}{\\partial v_c}=\\sum_{w=1}^V \\hat{y}_wu_w-u_o.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b**. \n",
    "$$\\frac{\\partial J}{\\partial U} = v_c(\\hat{y}-y)^\\text{T}$$\n",
    "Equivalently,\n",
    "$$\\frac{\\partial J}{\\partial u_k} = \n",
    "\\begin{cases}\n",
    "     (\\hat{y}_k-1)v_c, & k=o\\\\\n",
    "     \\hat{y}_kv_c, & k \\neq o\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c**.\n",
    "$$\\frac{\\partial J}{\\partial v_c}=\n",
    "-(1-\\sigma(u_o^\\text{T}v_c))u_o+\\sum_{k=1}^K(1-\\sigma(-u_k^\\text{T}v_c))u_k$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial u_w}= \\left\\{\n",
    "\\begin{array}{}\n",
    "-(1-\\sigma(u_w^\\text{T}v_c))v_c, & w=o\\\\\n",
    "(1-\\sigma(-u_w^\\text{T}v_c))v_c, & w=1, 2, ..., K\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "There are less terms using negative sampling loss compared to the softmax-CE loss; the speed-up ratio is $O(V/K)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d**.\n",
    "$$\\frac{\\partial J_{skip-gram}(w_{t-m...t+m})}{\\partial U}=\\sum_{-m\\le j \\le m, j\\neq 0}\\frac{\\partial F(w_{t+j}, v_c)}{\\partial U}$$\n",
    "\n",
    "$$\\frac{\\partial J_{skip-gram}(w_{t-m...t+m})}{\\partial v_k}= \\left\\{\n",
    "\\begin{array}{}\n",
    "\\sum_{-m\\le j \\le m, j \\neq 0}\\frac{\\partial F(w_{t+j},v_c)}{\\partial v_k}, & k = c\\\\\n",
    "0,&k \\neq c\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "$$\\frac{\\partial J_{CBOW}(w_{c-m...c+m})}{\\partial U}=\\frac{\\partial F(w_t, \\hat{v})}{\\partial U}$$\n",
    "\n",
    "$$\\frac{\\partial J_{CBOW}(w_{c-m...c+m})}{\\partial v_k}=\\left\\{\n",
    "\\begin{array}{}\n",
    "\\frac{\\partial F(w_t, \\hat{v})}{\\partial v_k}, & k=w_{t+j}, j \\in \\{-m, ...,-1, +1, ..., +m\\}\\\\\n",
    "0, & k \\neq w_{t+j}, j \\in \\{-m, ...,-1, +1, ..., +m\\}\\\\\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3e** and **3h**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[0.6        0.8       ]\n",
      " [0.4472136  0.89442719]]\n",
      "\n",
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(11.16610900153398, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(14.093692760899629, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-3.86802836, -1.12713967, -1.52668625],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.11265089,  0.05169237,  0.39321163],\n",
      "       [-0.22716495,  0.10423969,  0.79292674],\n",
      "       [-0.79674766,  0.36560539,  2.78107395],\n",
      "       [-0.31602611,  0.14501561,  1.10309954],\n",
      "       [-0.80620296,  0.36994417,  2.81407799]]))\n",
      "(0.798995801090665, array([[ 0.23330542, -0.51643128, -0.8281311 ],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80954933,  0.21962514, -0.54095764],\n",
      "       [-0.03556575, -0.00964874,  0.02376577],\n",
      "       [-0.13016109, -0.0353118 ,  0.08697634],\n",
      "       [-0.1650812 , -0.04478539,  0.11031068],\n",
      "       [-0.47874129, -0.1298792 ,  0.31990485]]))\n",
      "(7.895593203599139, array([[-2.98873309, -3.38440688, -2.62676289],\n",
      "       [-1.49436655, -1.69220344, -1.31338145],\n",
      "       [-1.49436655, -1.69220344, -1.31338145],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.21992784,  0.0596649 , -0.14696034],\n",
      "       [-1.37825047, -0.37390982,  0.92097553],\n",
      "       [-0.77702167, -0.21080061,  0.51922198],\n",
      "       [-2.58955401, -0.7025281 ,  1.73039366],\n",
      "       [-2.36749007, -0.64228369,  1.58200593]]))\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q3_word2vec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3f**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.414836786079764e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.524451035823933e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q3_sgd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3g**.\n",
    "\n",
    "![](q3_word_vectors.png)\n",
    "\n",
    "**Note**: My result here is different from that in solution, presumably because I used python 3 rather than the officially supported version of python (python 2), and I adapted some codes to make it compatible with python 3. The final loss is 9.4384, which passed the sanity check.\n",
    "\n",
    "**Explanation**: Words of similar meanings are \"clustered\" together. Some punctuations and frequent words are isolated from word clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4a**.\n",
    "```python\n",
    "def getSentenceFeatures(tokens, wordVectors, sentence):\n",
    "    \"\"\"\n",
    "    Obtain the sentence feature for sentiment analysis by averaging its\n",
    "    word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement computation for the sentence features given a sentence.\n",
    "\n",
    "    # Inputs:\n",
    "    # tokens -- a dictionary that maps words to their indices in\n",
    "    #           the word vector list\n",
    "    # wordVectors -- word vectors (each row) for all tokens\n",
    "    # sentence -- a list of words in the sentence of interest\n",
    "\n",
    "    # Output:\n",
    "    # - sentVector: feature vector for the sentence\n",
    "\n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    sentVector = np.mean([wordVectors[tokens[w]] for w in sentence], axis=0)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert sentVector.shape == (wordVectors.shape[1],)\n",
    "    return sentVector\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b**. We want to use regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c**.\n",
    "```python\n",
    "def getRegularizationValues():\n",
    "    \"\"\"Try different regularizations\n",
    "\n",
    "    Return a sorted list of values to try.\n",
    "    \"\"\"\n",
    "    # values = None   # Assign a list of floats in the block below\n",
    "    ### YOUR CODE HERE\n",
    "    values = [10 ** p for p in range(-6, 4)]\n",
    "    ### END YOUR CODE\n",
    "    return sorted(values)\n",
    "\n",
    "\n",
    "def chooseBestModel(results):\n",
    "    \"\"\"Choose the best model based on dev set performance.\n",
    "\n",
    "    Arguments:\n",
    "    results -- A list of python dictionaries of the following format:\n",
    "        {\n",
    "            \"reg\": regularization,\n",
    "            \"clf\": classifier,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy\n",
    "        }\n",
    "\n",
    "    Each dictionary represents the performance of one model.\n",
    "\n",
    "    Returns:\n",
    "    Your chosen result dictionary.\n",
    "    \"\"\"\n",
    "    bestResult = None\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    bestResult = max(results, key=lambda m:m['dev'])\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return bestResult\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4d**. First, we need to fix the encoding bug in datasetSentences.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate py27\n",
    "cd utils\n",
    "python fix_encoding.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, I manually fix the encoding bug in dictionary.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for reg=0.000001\n",
      "Train accuracy (%): 30.138109\n",
      "Dev accuracy (%): 29.700272\n",
      "Test accuracy (%): 29.230769\n",
      "Training for reg=0.000010\n",
      "Train accuracy (%): 30.102996\n",
      "Dev accuracy (%): 29.609446\n",
      "Test accuracy (%): 29.185520\n",
      "Training for reg=0.000100\n",
      "Train accuracy (%): 30.056180\n",
      "Dev accuracy (%): 29.700272\n",
      "Test accuracy (%): 29.185520\n",
      "Training for reg=0.001000\n",
      "Train accuracy (%): 30.044476\n",
      "Dev accuracy (%): 29.972752\n",
      "Test accuracy (%): 29.185520\n",
      "Training for reg=0.010000\n",
      "Train accuracy (%): 29.985955\n",
      "Dev accuracy (%): 30.426885\n",
      "Test accuracy (%): 28.959276\n",
      "Training for reg=0.100000\n",
      "Train accuracy (%): 29.552903\n",
      "Dev accuracy (%): 30.063579\n",
      "Test accuracy (%): 28.597285\n",
      "Training for reg=1.000000\n",
      "Train accuracy (%): 28.300562\n",
      "Dev accuracy (%): 26.521344\n",
      "Test accuracy (%): 26.108597\n",
      "Training for reg=10.000000\n",
      "Train accuracy (%): 27.212079\n",
      "Dev accuracy (%): 25.522252\n",
      "Test accuracy (%): 23.076923\n",
      "Training for reg=100.000000\n",
      "Train accuracy (%): 27.247191\n",
      "Dev accuracy (%): 25.522252\n",
      "Test accuracy (%): 23.031674\n",
      "Training for reg=1000.000000\n",
      "Train accuracy (%): 27.247191\n",
      "Dev accuracy (%): 25.522252\n",
      "Test accuracy (%): 23.031674\n",
      "\n",
      "=== Recap ===\n",
      "Reg\t\tTrain\tDev\tTest\n",
      "1.00E-06\t30.138\t29.700\t29.231\n",
      "1.00E-05\t30.103\t29.609\t29.186\n",
      "1.00E-04\t30.056\t29.700\t29.186\n",
      "1.00E-03\t30.044\t29.973\t29.186\n",
      "1.00E-02\t29.986\t30.427\t28.959\n",
      "1.00E-01\t29.553\t30.064\t28.597\n",
      "1.00E+00\t28.301\t26.521\t26.109\n",
      "1.00E+01\t27.212\t25.522\t23.077\n",
      "1.00E+02\t27.247\t25.522\t23.032\n",
      "1.00E+03\t27.247\t25.522\t23.032\n",
      "\n",
      "Best regularization value: 1.00E-02\n",
      "Test accuracy (%): 28.959276\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q4_sentiment.py --yourvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for reg=0.000001\n",
      "Train accuracy (%): 39.899345\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.000010\n",
      "Train accuracy (%): 39.969569\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.000100\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.001000\n",
      "Train accuracy (%): 39.899345\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.104072\n",
      "Training for reg=0.010000\n",
      "Train accuracy (%): 39.957865\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.100000\n",
      "Train accuracy (%): 39.782303\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.149321\n",
      "Training for reg=1.000000\n",
      "Train accuracy (%): 39.513109\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.420814\n",
      "Training for reg=10.000000\n",
      "Train accuracy (%): 38.611891\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.692308\n",
      "Training for reg=100.000000\n",
      "Train accuracy (%): 36.317884\n",
      "Dev accuracy (%): 35.059037\n",
      "Test accuracy (%): 35.701357\n",
      "Training for reg=1000.000000\n",
      "Train accuracy (%): 32.151217\n",
      "Dev accuracy (%): 31.244323\n",
      "Test accuracy (%): 30.588235\n",
      "\n",
      "=== Recap ===\n",
      "Reg\t\tTrain\tDev\tTest\n",
      "1.00E-06\t39.899\t36.512\t37.014\n",
      "1.00E-05\t39.970\t36.512\t36.968\n",
      "1.00E-04\t39.946\t36.421\t36.968\n",
      "1.00E-03\t39.899\t36.512\t37.104\n",
      "1.00E-02\t39.958\t36.240\t37.195\n",
      "1.00E-01\t39.782\t36.240\t37.149\n",
      "1.00E+00\t39.513\t36.512\t37.421\n",
      "1.00E+01\t38.612\t36.876\t37.692\n",
      "1.00E+02\t36.318\t35.059\t35.701\n",
      "1.00E+03\t32.151\t31.244\t30.588\n",
      "\n",
      "Best regularization value: 1.00E+01\n",
      "Test accuracy (%): 37.692308\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q4_sentiment.py --pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasons**:\n",
    "1. In this case, the dimension of GloVe vectors is higher than that of our word2vec vectors. More information could be encoded by higher dimensional word vectors.\n",
    "2. GloVe vectors used here were trained on a much larger corpus.\n",
    "3. GloVe has more efficient usage of global statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4e**.\n",
    "\n",
    "![](q4_reg_v_acc.png)\n",
    "\n",
    "Training accuracy decreases as regularization increases, while there is a peak for validation accuracy and validation accuracy decreases as regularization parameter deviates from the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4f**.\n",
    "\n",
    "![](q4_dev_conf.png)\n",
    "\n",
    "The sentiment for most samples is predicted correctly, although there are some mistakes. There are not many extreme predictions (i.e., very negative or very positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4g**. Example 1: true: 3; predicted: 1; text: but taken as a stylish and energetic one-shot , the queen of the damned can not be said to suck .  \n",
    "Explanation: the model was unable to handle negation.\n",
    "\n",
    "Example 2: true: 1; predicted: 3; text: too much of the humor falls flat .  \n",
    "Explanation: the model was unable to handle negation.\n",
    "\n",
    "Example 3: true: 1; predicted: 3; text:plays like a volatile and overlong w magazine fashion spread .\n",
    "Explanation: the model fails to capture word order and context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
