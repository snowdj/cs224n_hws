{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a**. Proof: $$\\text{softmax}(x+c)_i = \\frac{e^{x_i+c}}{\\sum_je^{x_j+c}}=\\frac{e^ce^{x_i}}{e^c\\sum_je^{x_j}}\n",
    "=\\frac{e^{x_i}}{\\sum_je^{x_j}}=\\text{softmax}(x)_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[0.26894142 0.73105858]\n",
      "[[0.26894142 0.73105858]\n",
      " [0.26894142 0.73105858]]\n",
      "[[0.73105858 0.26894142]]\n",
      "You should be able to verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python q1_softmax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Neural Network Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a**.\n",
    "$$\\sigma'(x)=\\frac{-1}{(1+e^{-x})^2}\\cdot(-e^{-x})=\\frac{1}{1+e^{-x}}\\cdot\\frac{e^{-x}}{1+e^{-x}}\n",
    "=\\sigma(x)(1-\\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b**. Assume that only the k-th dimension of $\\boldsymbol{y}$ is 1 and others are 0. We have\n",
    "$$CE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}})=-y_k\\log (\\hat{y}_k)=-y_k(\\log e^{\\theta_k}-\\log \\sum_i e^{\\theta_i}) = \\log \\sum_i e^{\\theta_i} - \\theta_k$$\n",
    "$$\\frac{\\partial CE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}})}{\\partial\\theta_k}\n",
    "=\\frac{e^{\\theta_k}}{\\sum_i e^{\\theta_i}}-1=\\hat{y}_k-1$$\n",
    "For $j \\neq k$,\n",
    "$$\\frac{\\partial CE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}})}{\\partial\\theta_j}\n",
    "=\\frac{e^{\\theta_j}}{\\sum_i e^{\\theta_i}}=\\hat{y}_j$$\n",
    "\n",
    "$$\\therefore \\frac{\\partial CE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}})}{\\partial\\boldsymbol{\\theta}}=\\boldsymbol{\\hat{y}}-\\boldsymbol{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c**. Denote $z_1=xW_1+b_1$ and $z_2=hW_2+b_2$, then\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial x} = (\\frac{\\partial J}{\\partial z_2}\\frac{\\partial z_2}{\\partial h}\\odot\\frac{\\partial h}{\\partial z_1})\\frac{\\partial z_1}{\\partial x}\n",
    "=((\\hat{y}-y)W_2^\\text{T}\\odot(h(1-h))W_1^\\text{T})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d**. The total number of parameters is $(D_x+1)H+(H+1)D_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[0.73105858 0.88079708]\n",
      " [0.26894142 0.11920292]]\n",
      "[[0.19661193 0.10499359]\n",
      " [0.19661193 0.10499359]]\n",
      "You should verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python q2_sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2f**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python q2_gradcheck.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2g**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python q2_neural.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a**. We know $u_W$ and $v_w$ are column vectors of size, say $H$. The the shape of $U$ is $(H, V)$. Let $\\hat{y}$ be the column vector of softmax predictions, and $y$ be the one-hot labels as a column vector. Then,\n",
    "$$\\frac{\\partial J}{\\partial v_c}=U(\\hat{y}-y)$$\n",
    "The elements $\\hat{y}_w$ and $y_w$ of $\\hat{y}$ and $y$ are scalers. Thus equivalently,\n",
    "$$\\frac{\\partial J}{\\partial v_c}=\\sum_{w=1}^V \\hat{y}_wu_w-u_o.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b**. \n",
    "$$\\frac{\\partial J}{\\partial U} = v_c(\\hat{y}-y)^\\text{T}$$\n",
    "Equivalently,\n",
    "$$\\frac{\\partial J}{\\partial u_k} = \\left\\{\n",
    "\\begin{array}{}\n",
    "     (\\hat{y}_k-1)v_c, & k=o\\\\\n",
    "     \\hat{y}_kv_c, & k \\neq o\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c**.\n",
    "$$\\frac{\\partial J}{\\partial v_c}=\n",
    "-(1-\\sigma(u_o^\\text{T}v_c))u_o+\\sum_{k=1}^K(1-\\sigma(-u_k^\\text{T}v_c))u_k$$\n",
    "$$\\frac{\\partial J}{\\partial u_w}= \\left\\{\n",
    "\\begin{array}{}\n",
    "-(1-\\sigma(u_w^\\text{T}v_c))v_c, & w=o\\\\\n",
    "(1-\\sigma(-u_w^\\text{T}v_c))v_c, & w=1, 2, ..., K\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "There are less terms using negative sampling loss compared to the softmax-CE loss; the speed-up ratio is $O(V/K)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d**.\n",
    "$$\\frac{\\partial J_{skip-gram}(w_{t-m...t+m})}{\\partial U}=\\sum_{-m\\le j \\le m, j\\neq 0}\\frac{\\partial F(w_{t+j}, v_c)}{\\partial U}$$\n",
    "\n",
    "$$\\frac{\\partial J_{skip-gram}(w_{t-m...t+m})}{\\partial v_k}= \\left\\{\n",
    "\\begin{array}{}\n",
    "\\sum_{-m\\le j \\le m, j \\neq 0}\\frac{\\partial F(w_{t+j},v_c)}{\\partial v_k}, & k = c\\\\\n",
    "0,&k \\neq c\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "$$\\frac{\\partial J_{CBOW}(w_{c-m...c+m})}{\\partial U}=\\frac{\\partial F(w_t, \\hat{v})}{\\partial U}$$\n",
    "\n",
    "$$\\frac{\\partial J_{CBOW}(w_{c-m...c+m})}{\\partial v_k}=\\left\\{\n",
    "\\begin{array}{}\n",
    "\\frac{\\partial F(w_t, \\hat{v})}{\\partial v_k}, & k=w_{t+j}, j \\in \\{-m, ...,-1, +1, ..., +m\\}\\\\\n",
    "0, & k \\neq w_{t+j}, j \\in \\{-m, ...,-1, +1, ..., +m\\}\\\\\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3e** and **3h**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[0.6        0.8       ]\n",
      " [0.4472136  0.89442719]]\n",
      "\n",
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(11.16610900153398, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(14.093692760899629, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-3.86802836, -1.12713967, -1.52668625],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.11265089,  0.05169237,  0.39321163],\n",
      "       [-0.22716495,  0.10423969,  0.79292674],\n",
      "       [-0.79674766,  0.36560539,  2.78107395],\n",
      "       [-0.31602611,  0.14501561,  1.10309954],\n",
      "       [-0.80620296,  0.36994417,  2.81407799]]))\n",
      "(0.798995801090665, array([[ 0.23330542, -0.51643128, -0.8281311 ],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80954933,  0.21962514, -0.54095764],\n",
      "       [-0.03556575, -0.00964874,  0.02376577],\n",
      "       [-0.13016109, -0.0353118 ,  0.08697634],\n",
      "       [-0.1650812 , -0.04478539,  0.11031068],\n",
      "       [-0.47874129, -0.1298792 ,  0.31990485]]))\n",
      "(7.895593203599139, array([[-2.98873309, -3.38440688, -2.62676289],\n",
      "       [-1.49436655, -1.69220344, -1.31338145],\n",
      "       [-1.49436655, -1.69220344, -1.31338145],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.21992784,  0.0596649 , -0.14696034],\n",
      "       [-1.37825047, -0.37390982,  0.92097553],\n",
      "       [-0.77702167, -0.21080061,  0.51922198],\n",
      "       [-2.58955401, -0.7025281 ,  1.73039366],\n",
      "       [-2.36749007, -0.64228369,  1.58200593]]))\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python q3_word2vec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3f**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.414836786079764e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.524451035823933e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python q3_sgd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3g**.\n",
    "\n",
    "![](q3_word_vectors.png)\n",
    "\n",
    "**Note**: My result here is different from that in solution, presumably because I used python 3 rather than the officially supported version of python (python 2), and I adapted some codes to make it compatible with python 3. The final loss is 9.8122, which passed the sanity check.\n",
    "\n",
    "**Explanation**: Words of similar meanings are \"clustered\" together. Some punctuations and frequent words are isolated from word clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
