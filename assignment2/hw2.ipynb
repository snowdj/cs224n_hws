{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I adapted the starter codes to make them compatible with python 3. All codes were run in python 3 (rather than the officially supported version of python, i.e., python 2) unless clarified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Tensorflow Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a** and **1b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax test 1 passed!\n",
      "Softmax test 2 passed!\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n",
      "Cross-entropy test 1 passed!\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q1_softmax.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c**. Placeholder variables and feed dictionaries make it possible to feed data into the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1d** and **1e**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 59.18 (0.114 sec)\n",
      "Epoch 1: loss = 20.32 (0.009 sec)\n",
      "Epoch 2: loss = 10.92 (0.008 sec)\n",
      "Epoch 3: loss = 7.30 (0.008 sec)\n",
      "Epoch 4: loss = 5.44 (0.008 sec)\n",
      "Epoch 5: loss = 4.32 (0.009 sec)\n",
      "Epoch 6: loss = 3.58 (0.009 sec)\n",
      "Epoch 7: loss = 3.05 (0.009 sec)\n",
      "Epoch 8: loss = 2.65 (0.008 sec)\n",
      "Epoch 9: loss = 2.35 (0.008 sec)\n",
      "Epoch 10: loss = 2.11 (0.008 sec)\n",
      "Epoch 11: loss = 1.91 (0.007 sec)\n",
      "Epoch 12: loss = 1.75 (0.007 sec)\n",
      "Epoch 13: loss = 1.61 (0.007 sec)\n",
      "Epoch 14: loss = 1.49 (0.007 sec)\n",
      "Epoch 15: loss = 1.39 (0.007 sec)\n",
      "Epoch 16: loss = 1.30 (0.007 sec)\n",
      "Epoch 17: loss = 1.22 (0.007 sec)\n",
      "Epoch 18: loss = 1.15 (0.006 sec)\n",
      "Epoch 19: loss = 1.09 (0.007 sec)\n",
      "Epoch 20: loss = 1.03 (0.007 sec)\n",
      "Epoch 21: loss = 0.98 (0.007 sec)\n",
      "Epoch 22: loss = 0.94 (0.006 sec)\n",
      "Epoch 23: loss = 0.89 (0.007 sec)\n",
      "Epoch 24: loss = 0.86 (0.006 sec)\n",
      "Epoch 25: loss = 0.82 (0.006 sec)\n",
      "Epoch 26: loss = 0.79 (0.007 sec)\n",
      "Epoch 27: loss = 0.76 (0.006 sec)\n",
      "Epoch 28: loss = 0.73 (0.006 sec)\n",
      "Epoch 29: loss = 0.71 (0.006 sec)\n",
      "Epoch 30: loss = 0.68 (0.006 sec)\n",
      "Epoch 31: loss = 0.66 (0.006 sec)\n",
      "Epoch 32: loss = 0.64 (0.007 sec)\n",
      "Epoch 33: loss = 0.62 (0.006 sec)\n",
      "Epoch 34: loss = 0.60 (0.007 sec)\n",
      "Epoch 35: loss = 0.58 (0.006 sec)\n",
      "Epoch 36: loss = 0.57 (0.006 sec)\n",
      "Epoch 37: loss = 0.55 (0.006 sec)\n",
      "Epoch 38: loss = 0.54 (0.006 sec)\n",
      "Epoch 39: loss = 0.52 (0.006 sec)\n",
      "Epoch 40: loss = 0.51 (0.006 sec)\n",
      "Epoch 41: loss = 0.50 (0.006 sec)\n",
      "Epoch 42: loss = 0.48 (0.006 sec)\n",
      "Epoch 43: loss = 0.47 (0.006 sec)\n",
      "Epoch 44: loss = 0.46 (0.006 sec)\n",
      "Epoch 45: loss = 0.45 (0.006 sec)\n",
      "Epoch 46: loss = 0.44 (0.006 sec)\n",
      "Epoch 47: loss = 0.43 (0.007 sec)\n",
      "Epoch 48: loss = 0.42 (0.006 sec)\n",
      "Epoch 49: loss = 0.41 (0.006 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q1_classifier.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During forward propagation, the loss gets computed; during backpropagation, the derivatives with respect to the variables in the graph get computed; after the op has been run, the variables in the graph will be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Neural Transition-Based Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a**.\n",
    "\n",
    "stack| buffer | new dependency | transition\n",
    "--- | --- | ---| ---\n",
    "[ROOT] | [I, parsed, this, sentence, correctly] | | Initial Configuration\n",
    "[ROOT, I] | [parsed, this, sentence, correctly] | | SHIFT\n",
    "[ROOT, I, parsed] | [this, sentence, correctly] | | SHIFT\n",
    "[ROOT, parsed] | [this, sentence, correctly] | parsed → I | LEFT-ARC\n",
    "[ROOT, parsed, this] | [sentence, correctly] | | SHIFT\n",
    "[ROOT, parsed, this, sentence] | [correctly] | | SHIFT\n",
    "[ROOT, parsed, sentence] | [correctly] | sentence → this | LEFT-ARC\n",
    "[ROOT, parsed] | [correctly] | parsed → sentence | RIGHT-ARC\n",
    "[ROOT, parsed, correctly] | [] | | SHIFT\n",
    "[ROOT, parsed] | [] | parsed → correctly | RIGHT-ARC\n",
    "[ROOT] | [] | ROOT → parsed | RIGHT-ARC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b**. A sentence containing $n$ words will be parsed in $2n$ steps; each word must be shifted onto the stack and moved away through LEFT-ARC or RIGHT-ARC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c** and **2d**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHIFT test passed!\n",
      "LEFT-ARC test passed!\n",
      "RIGHT-ARC test passed!\n",
      "parse test passed!\n",
      "minibatch_parse test passed!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q2_parser_transitions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2e**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "Basic (non-exhaustive) Xavier initialization tests pass\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q2_initialization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2f**.\n",
    "\n",
    "$$\\gamma = \\frac{1}{1 - p_{drop}}$$\n",
    "\n",
    "$$\\mathbb{E}_{p_{drop}}[h_{drop}]_i = \\mathbb{E}_{p_{drop}}[\\gamma d_i h_i] = p_{drop}\\cdot 0 + (1-p_{drop})\\cdot \\gamma h_i = (1-p_{drop})\\cdot \\gamma h_i = h_i$$\n",
    "\n",
    "$$\\therefore \\gamma = \\frac{1}{1 - p_{drop}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2g**. **(i)** Each update depends on the previous update since the previous $m$ would be multiplied by $\\beta_1$ and the current gradient would be multiplied by $(1 - \\beta_1)$. The rolling average is close to calculating the gradient over a larger minibatch, the variance of each update would be reduced, and each update would be closer to the gradient over the whole dataset.\n",
    "\n",
    "**(ii)** The parameters with the smaller gradients (on average) will get larger updates; this would help parameters move out of the flat areas (saddle points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2h**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 1.14 seconds\n",
      "Building parser...\n",
      "took 0.02 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 1.29 seconds\n",
      "Vectorizing data...\n",
      "took 0.03 seconds\n",
      "Preprocessing training data...\n",
      "took 0.79 seconds\n",
      "Building model...\n",
      "took 0.11 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.6002\n",
      "Evaluating on dev set\n",
      "- dev UAS: 55.74\n",
      "\n",
      "Epoch 2 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.3374\n",
      "Evaluating on dev set\n",
      "- dev UAS: 60.93\n",
      "\n",
      "Epoch 3 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.2770\n",
      "Evaluating on dev set\n",
      "- dev UAS: 63.93\n",
      "\n",
      "Epoch 4 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.2436\n",
      "Evaluating on dev set\n",
      "- dev UAS: 65.97\n",
      "\n",
      "Epoch 5 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.2169\n",
      "Evaluating on dev set\n",
      "- dev UAS: 68.35\n",
      "\n",
      "Epoch 6 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.1938\n",
      "Evaluating on dev set\n",
      "- dev UAS: 69.00\n",
      "\n",
      "Epoch 7 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.1763\n",
      "Evaluating on dev set\n",
      "- dev UAS: 70.32\n",
      "\n",
      "Epoch 8 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.1635\n",
      "Evaluating on dev set\n",
      "- dev UAS: 72.07\n",
      "\n",
      "Epoch 9 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.1525\n",
      "Evaluating on dev set\n",
      "- dev UAS: 71.51\n",
      "\n",
      "Epoch 10 out of 10\n",
      "48/48 [============================>.] - ETA: 0s - train loss: 0.1433\n",
      "Evaluating on dev set\n",
      "- dev UAS: 72.12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source activate py36\n",
    "python q2_run_h1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best UAS on the dev set is 88.71; the UAS on the test set is 89.10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2i**. I added an additional hidden layer. In principle, an additional hidden layer increases the expressibility of the model due to the increased ability in catching nonlinearities; but it makes the model harder to train.\n",
    "\n",
    "The best UAS on the dev set is 88.40; the UAS on the test set is 88.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Recurrent Neural Networks: Language Modeling\n",
    "\n",
    "(See solution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
